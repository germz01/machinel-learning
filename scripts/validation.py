from __future__ import division

import csv
import nn
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import random
import utils as u
import nn as NN
import json

from itertools import product
from sklearn.utils.extmath import cartesian
from tqdm import tqdm


class KFoldCrossValidation(object):
    """
    This class represents a wrapper for the implementation of the classic
    k-fold cross validation algorithm, as described in Deep Learning pag.
    120.

    Attributes
    ----------
    dataset: numpy.ndarray
        the full dataset obtained by stiking the design matrix X with the
        target column vector y

    folds: list
        a list containing nfold chunks of the original dataset

    results: list
        a list containing the generalization assessment measures
        generated by every algorithm's iteration for each fold.

    aggregated_results: dict
        a dict containing assessment values and aggregates for each metric

    """

    def __init__(self, X, y, neural_net, nfolds=3, shuffle=False, **kwargs):
        """
        The class' constructor.

        Parameters
        ----------
        X: numpy.ndarray
            the design matrix

        y: numpy.ndarray
            the target column vector

        neural_net: nn.NeuralNetwork
            the neural network that has to be cross validated

        nfolds: int
            the number of folds to be applied in the algorithm
            (Default value = 3)

        shuffle: bool
             choosing if the dataset must be shuffled
                    (Default value = False)

        kwargs: dict
            a dictionary which contains the parameters for the neural
            network's initialization

        Returns
        -------
        """
        assert X.shape[0] == y.shape[0]
        self.X_dim = X.shape[1]  # added X_dim for splitting train/target
        self.dataset = np.hstack((X, y))
        self.folds = list()
        self.results = list()

        if shuffle:
            np.random.selfhuffle(self.dataset)
        self.set_folds(nfolds)
        self.validate(X, y, neural_net, nfolds)

    def set_folds(self, nfolds):
        """
        This function splits the dataset into nfolds folds.

        Parameters
        ----------
        nfolds: int
            the number of folds to be applied in the algorithm

        Returns
        -------
        """
        record_per_fold = int(self.dataset.shape[0] / nfolds)
        low = 0
        high = low + record_per_fold

        for i in np.arange(nfolds):
            self.folds.append(self.dataset[low:high] if i !=
                              nfolds - 1 else self.dataset[low:])

            low = high
            high += record_per_fold

    def validate(self, X, y, neural_net, nfolds, plot_curves=False, **kwargs):
        """
        This function implements the core of the k-fold cross validation
        algorithm. For each fold, the neural network is trained using the
        training set created for that fold, and is tested on the respective
        test set. Finally, the error between the test's target and the
        predicted one is collected.

        Parameters
        ----------
        X. numpy.ndarray
            the design matrix

        y: numpy.ndarray
            the target column vector

        neural_net: nn.NeuralNetwork
            the neural network that has to be cross validated

        nfolds: int
            the number of folds to be applied in the algorithm

        plot_curves: bool
            whether or not to plot the learning curve for each one of the
            cross validation's iterations

        kwargs: dict
            a dictionary which contains the parameters for the neural
            network's initialization

        Returns
        -------
        """
        for i in tqdm(np.arange(nfolds),
                      desc='{}-FOLD CROSS VALIDATION PROGRESS'.format(nfolds)):

            train_set = np.vstack([self.folds[j] for j in np.arange(
                len(self.folds)) if j != i])

            X_train, y_train = np.hsplit(train_set, [self.X_dim])
            X_va, y_va = np.hsplit(self.folds[i], [self.X_dim])

            neural_net.train(X_train, y_train)

            assessment = self.model_assessment(X_va, y_va, model=neural_net)
            self.results.append(assessment)
            # self.results.append(loss)
            neural_net.reset()

            # TODO: fix plots, adjusting for new modifications
            if plot_curves:
                plt.plot(range(len(neural_net.error_per_epochs)),
                         neural_net.error_per_epochs,
                         label='FOLD {}, VALIDATION ERROR: {}'.
                         format(i, round(loss, 2)))

        self.aggregated_results = self.aggregate_assessments()
        return self.aggregated_results

        if plot_curves:
            plt.title('LEARNING CURVES FOR A {}-FOLD CROSS VALIDATION.\nMEAN '
                      'VALIDATION ERROR {}, VARIANCE {}.'.
                      format(nfolds, round(self.mean_result, 2),
                             round(self.std_result, 2)),
                      fontsize=8)
            plt.ylabel('ERROR PER EPOCH')
            plt.xlabel('EPOCHS')
            plt.grid()
            plt.legend(fontsize=8)
            plt.savefig('../images/{}_fold_cross_val_lcs.pdf'.
                        format(nfolds), bbox_inches='tight')
            plt.close()

    def model_assessment(self, X_va, y_va, model):
        """
        Computes assessment measures for each fold evaluation.

        Parameters
        ----------
        model : type
            model object

        X_va, y_va: numpy.ndarray
            Validation datasets.

        Returns
        -------
        assessment : dict
            dictionary with structure { metric: estimated value}.
        """

        assessment = dict()
        assessment['mse'] = model.predict(X_va, y_va)
        # possibile aggiungere altre metriche al dizionario
        return assessment

    def aggregate_assessments(self):
        """
        Computes aggregation measures for each assessment metric.

        Parameters
        ----------

        Returns
        -------
        out : dict
            dictionary containing folds results and aggregates.
            out = {metric: {'mean': 0, 'std': 0, 'median': 0, 'values': []}}

        """

        metrics = self.results[0].keys()

        out = {metric: {'values': []} for metric in metrics}
        for res in self.results:
            for metric in metrics:
                out[metric]['values'].append(res[metric])

        for metric in metrics:
            out[metric]['mean'] = np.mean(out[metric]['values'])
            out[metric]['std'] = np.std(out[metric]['values'])
            out[metric]['median'] = np.median(out[metric]['values'])

        return out


class ModelSelectionCV():
    """ Model selection using repeated Cross Validation """
    def __init__(self, X_design, y_design, grid,
                 nfolds=3, repetitions=1,
                 save_results=True,
                 fname='model_selection_results.json'):
        """
        Selection of the best model by grid search with Cross Validation

        Parameters
        ----------
        X_design, y_design : numpy.ndarray
            design dataset

        grid:
            grid object

        repetitions: int
            Cross Validation Repetitions
            (Default value = 1)

        Returns
        -------
        """

        self.X_design = X_design
        self.y_design = y_design
        self.grid = grid
        self.repetitions = repetitions
        self.fname = fname
        self.n_iter = repetitions*grid.N
        self.save_results = save_results
        self.nfolds = nfolds

    def search(self):
        """
        What it does?

        Parameters
        ----------
        par : type
        par_description

        Returns
        -------
        out : type
        out_description
        """

        if self.save_results:
            with open(self.fname, 'w') as f:
                f.write('{"out":[ ')
        i = 0
        for rep in range(self.repetitions):

            dataset = np.hstack((self.X_design, self.y_design))
            np.random.shuffle(dataset)
            X_design, y_design = np.hsplit(dataset,
                                           [self.X_design.shape[1]])

            for hyperparams in tqdm(self.grid, desc='GRID SEARCH PROGRESS'):
                # instanciate neural network
                # TODO(?): generalize instanciation to any model
                neural_net = NN.NeuralNetwork(X_design, y_design,
                                              **hyperparams)

                cross_val = KFoldCrossValidation(
                    X_design, y_design, neural_net,
                    nfolds=self.nfolds, shuffle=False)

                i += 1
                out = dict()
                out['hyperparams'] = neural_net.get_params()
                out['errors'] = cross_val.aggregated_results

                with open(self.fname, 'a') as f:
                    json.dump(out, f)
                    if i != self.n_iter:
                        f.write(',\n')
                    else:
                        f.write('\n]}')

    def load_results(self):
        """
        What it does?

        Parameters
        ----------
        par : type
        par_description

        Returns
        -------
        out : type
        out_description
        """

        with open(self.fname, 'r') as f:
            data = json.load(f)
        return data

    def select_best_hyperparams(self, error='mse',
                                metric='mean', top=1):
        """
        Selection of the best hyperparameters

        Parameters
        ----------
        error: str
            error used

        metric: str

        top: int
            number of best hyperparameters
            (Default value = 1)

        Returns
        -------
        out : type
        out_description
        """

        data = self.load_results()
        errors = [res['errors'][error][metric] for res in data['out']]

        best_indexes = (np.argsort(errors))[:top]
        return list(np.array(data['out'])[best_indexes])

    def select_best_model(self):
        """ Retraining best model """
        best = self.select_best_hyperparams(top=1)
        best_hyperparams = best[0]['hyperparams']

        neural_net = NN.NeuralNetwork(self.X_design, self.y_design,
                                      **best_hyperparams)
        neural_net.train(self.X_design, self.y_design)
        return neural_net


class Holdout():
    """ Validation Holdout method """
    def __init__(self, X, y, split_perc=[0.5, 0.25, 0.25]):
        """
        Initialization for the Holdout class

        Parameters
        ----------
        X : numpy.ndarray
        y : numpy.ndarray
        split_perc : list
            split percentages

        Returns
        -------

        """
        df = np.hstack((X, y))
        np.random.shuffle(df)

        p = df.shape[0]
        tr_perc = split_perc[0]
        va_perc = split_perc[1]
        # ts_perc = split_perc[2]

        split_train = int(tr_perc*p)
        split_design = int((tr_perc+va_perc)*p)

        design_set = df[:split_design, :]
        train_set = df[:split_train, :]
        validation_set = df[split_train:split_design, :]
        test_set = df[split_design:, :]

        self.X_design, self.y_design = np.hsplit(design_set, [X.shape[1]])
        self.X_train, self.y_train = np.hsplit(train_set, [X.shape[1]])
        self.X_va, self.y_va = np.hsplit(validation_set, [X.shape[1]])
        self.X_test, self.y_test = np.hsplit(test_set, [X.shape[1]])

    def model_selection(self, grid, plot=False, fpath='../images/'):
        """
        Holdout model selection

        Parameters
        ----------
        grid : instance of HyperRandomGrid class
            hyperparameter grid
        plot : bool
            if plot=True plots the learning curve for each grid parameter

        fpath : str
            path for images storing
        Returns
        -------
        neural network object
        """

        self.fpath = fpath
        params = []
        errors_va = []
        for i, pars in enumerate(grid):
            # TODO: handle multiple layers
            nn = nn.NeuralNetwork(self.X_train, self.y_train, **pars)
            nn.train(self.X_train, self.y_train)
            print('trained')
            params.append(nn.get_params())
            # assess on validation set
            errors_va.append(
                nn.predict(self.X_va, self.y_va)/(self.X_va.shape[0])
            )
            if plot is True:
                u.plot_error(nn, fname=fpath
                             + 'learning_curve_{}.png'.format(i))

        # choosing the best hyperparameters
        self.best_index = np.argmin(errors_va)
        best_hyperparams = params[self.best_index]

        # retraining on design set
        nn_retrained = nn.NeuralNetwork(hidden_sizes=best_hyperparams
                                        .pop('hidden_sizes'))
        nn_retrained.train(self.X_design, self.y_design, **best_hyperparams)

        df_pars = pd.DataFrame(list(grid))
        df_pars['error'] = errors_va

        self.best_hyperparams = best_hyperparams
        self.df_pars = df_pars
        self.model = nn_retrained

        return self.model


class HyperRandomGrid():
    """ HyperRandomGrid """

    def __init__(self, param_ranges, N, seed=None):
        """
        HyperRandomGrid instanciates an iterator
        which produce random parameters, in the given ranges.

        The grid iterator is reset after each use,
        allowing immediate reuse of the same grid.

        Parameters
        ----------
        param_ranges : dict
        dictionary containing ranges interval for each parameter.

        N: int
        size of the grid.

        seed: int
        random seed initialization.

        Returns
        -------

        """

        self.N = N
        self.n = 0
        if type(param_ranges) is not dict:
            raise TypeError("Insert a dictionary of parameters ranges")
        self.param_ranges = param_ranges
        self.types = self.get_types()

        if seed is not None:
            # seed inizialization
            self.seed = seed
            random.seed(self.seed)
        else:
            # random initialization
            random.seed()
            self.seed = random.randint(0, 2**32)
            random.seed(self.seed)
            
    def get_types(self):
        """
        Get the type of each parameter

        Parameters
        ----------

        Returns
        -------
        types : dict
        dictionary containing each parameter type
        """

        types = dict()
        for par, interval in self.param_ranges.items():
            if (type(interval) is int) or \
               (type(interval) is float) or \
               (type(interval) is str):
                types[par] = 'constant'
            elif type(interval) is list:
                types[par] = list
            elif type(interval[0]) is int and type(interval[1] is int):
                types[par] = int
            elif type(interval[0]) is float and type(interval[1] is float):
                types[par] = float
            else:
                raise TypeError('Check interval type')
        return types

    def __iter__(self):
        return self

    def next(self):
        """
        Iterator next method,
        returns the next grid record

        Parameters
        ----------
        Returns
        -------
        x_grid : dict
        Randomized parameter dictionary
        """
        if self.n == 0:
            random.seed(self.seed)

        x_grid = dict()
        for par, interval in self.param_ranges.items():
            if self.types[par] is int:
                x_grid[par] = random.randint(interval[0], interval[1])
            elif self.types[par] is float:
                x_grid[par] = random.uniform(interval[0], interval[1])
            elif self.types[par] is list:
                x_grid[par] = []
                for el in interval:
                    if (type(el) is int):
                        x_grid[par].append(el)
                    elif type(el) is tuple:
                        x_grid[par].append(random.randint(el[0], el[1]))
            elif self.types[par] == 'constant':
                x_grid[par] = interval

        self.n += 1
        if self.n == self.N+1:
            self.n = 0
            # set random seed at exit
            random.seed()
            raise StopIteration
        else:
            return x_grid

    def reset_grid(self):
        """Reset the grid, to use again the iterator """
        random.seed(self.seed)
        self.n = 0

    def get_par_index(self, index):
        self.reset_grid()
        for i in range(index+1):
            params = self.next()
        return params


class HyperGrid():
    """
    HyperGrid class  instanciates a grid iterator object

    Attributes
    ----------
    grid_f: dict
        A dictionary containing function to generate parameters arrays
        using the input ranges

    """
    grid_f = {
        'linspace':
        {
            int: lambda start, end, num:
            np.linspace(start, end, num, dtype=int),
            float: lambda start, end, num:
            np.linspace(start, end, num, dtype=float)
        },
        'random':
        {
            int: lambda start, end, num:
            np.random.randint(start, end, num, dtype=int),
            float: lambda start, end, num:
            np.random.uniform(start, end, num)
        }
    }

    def __init__(self, param_ranges, size, method, ):
        """
        Initialize

        Parameters
        ----------
        param_ranges : dict
            dictionary containing hyperparameters ranges

        size : int
            size of each hyperparameter array

        method : str
            'linspace' to generate a uniform grid space
            'random' to generate a random grid

        Returns
        -------

        """

        self.param_ranges = param_ranges
        self.size = size
        self.method = method
        self.params = param_ranges.keys()
        self.params_index = {par: self.params.index(par)
                             for par in self.params}
        # set seed for the hyperGrid object
        self.seed = random.randint(0, 2**32)
        self.grid = self.get_grid()

        print('GENERATING AN HYPERPARAMETER GRID OF LENGTH {}'
              .format(self.get_grid_size()))

    def get_grid(self):
        """
        Produces the grid iterator

        Returns
        -------
        grid_iter: itertools.product
            a grid iterator
        """
        np.random.seed(seed=self.seed)
        par_vectors = dict()
        for par, interval in self.param_ranges.items():
            par_vectors[par] = self.grid_f[self.method][type(interval[0])](
                interval[0],
                interval[1],
                self.size
            )
            # return par_vectors
        grid_iter = product(*list(par_vectors.values()))
        return grid_iter

    def get_grid_list(self):
        """ get the grid as a list """
        grid_list = list(self.get_grid())

        return grid_list

    def get_grid_dict(self):
        """ get the grid as a dict """
        grid_iter = self.get_grid()
        grid_dict = {par: [] for par in self.params}
        for par_values in grid_iter:
            for par in self.params:
                grid_dict[par].append(
                    par_values[self.params_index[par]]
                )
        return grid_dict

    def get_grid_size(self):
        """ get the grid size """
        return self.size**len(self.params)

    def __iter__(self):
        """
        iterator method
        """
        return self

    def next(self):
        """
        Return next grid parameters as a dictionary
        """
        out = self.grid.next()

        d = dict()
        for i, par in enumerate(self.params):
            d[par] = out[i]

        return d

    def reset_grid(self):
        """
        Re-create the grid iterator, maintaining the same
        parameters
        """
        self.grid = self.get_grid()


class GridSearch(object):
    """
    This class is a wrapper for the grid search algorithm for the
    hyperparameters' optimization. It is possible to randomize the whole
    process by settig the random_search variable to True.

    Attributes
    ----------
    grid: numpy.ndarray
        A matrix in which each row represents a possible combination of
        hyperparameters that has to be tested

    results: dict
        A dictionary in which the keys are the mean error scores returned by
        the k-fold cross validation phase and the values are the
        hyperparameters that have produced that errors

    best_result: dict
        The best result returned by the search phase
    """

    def __init__(self, X, y, random_search=False, save_results=False,
                 **kwargs):
        """
        The class' constructor.

        Parameters
        ----------
        X: numpy.ndarray
            the design matrix

        y: numpy.ndarray
            the target column vector

        random_search: bool
            whether or not to proceed with a random hyperparameters's
            initialization
            (Default value = False)

        save_results: bool
            whether or not so save the results deriving from the search phase
            in a csv file

        kwargs: dict
            either the parameters' range if random_search is True or
            the parameters' records if random_search is False

        Returns
        -------
        """
        self.grid = self.set_grid(random_search, kwargs['par_ranges'] if
                                  random_search else kwargs['parameters'])
        self.results = self.search(X, y)
        self.best_result = {'error': min(self.results.keys()),
                            'parameters': self.results[min(
                                              self.results.keys())]}

        if save_results:
            self.save_results()

    def set_grid(self, random_search, par_or_par_ranges, num_par=3):
        """
        This function initializes the algorithm's grid attribute.

        Parameters
        ----------
        random_search: bool
            whether or not to proceed with a random hyperparameters's
            initialization
            (Default value = False)

        par_or_par_ranges: dict
            a dictionary containing either the parameters' ranges if
            random_search is True or the parameters' records if random_search
            is False

        num_par: int
            the length of the parameters' records
            (Default value = 3)

        Returns
        -------
        The grid as a numpy.ndarray.
        """
        grid = None

        if random_search:
            grid = HyperRandomGrid(par_or_par_ranges, num_par)
            grid = u.from_dict_to_list(grid)
        else:
            grid = u.from_dict_to_list(par_or_par_ranges)

        grid = cartesian([grid['eta'], grid['alpha'], grid['reg_lambda'],
                          grid['batch_size'], grid['epochs']])

        return grid

    def search(self, X, y, **kwarg):
        """
        This functions implements the search phase of the grid search
        algorithm.

        Parameters
        ----------
        X: numpy.ndarray
            the design matrix

        y: numpy.ndarray
            the target column vector

        kwargs: dict

        Returns
        -------
        A dictionary in which the keys are the mean error scores returned by
        the k-fold cross validation phase and the values are the
        hyperparameters that have produced that errors.
        """
        neural_net = nn.NeuralNetwork(X, y)
        results = dict()

        for record in tqdm(self.grid, desc='GRID SEARCH PROGRESS'):
            cross_val = KFoldCrossValidation(
                X, y, neural_net, eta=record[0],
                alpha=record[1], reg_method='l2', reg_lambda=record[2],
                batch_size=int(record[3]), epochs=int(record[4]))

            results[cross_val.mean_result] = {'eta': record[0],
                                              'alpha': record[1],
                                              'reg_lambda': record[2],
                                              'batch_size': int(record[3]),
                                              'epochs': int(record[4])}

        return results

    def save_results(self):
        """
        This functions writes the results deriving from the search phase in a
        csv file, and saves the resulting file in ../datasets.

        Parameters
        ----------

        Returns
        -------
        """
        with open('../datasets/grid_search_results.csv', 'wb') as csvfile:
            fieldnames = ['ETA', 'ALPHA', 'LAMBDA', 'BATCH SIZE', 'EPOCHS',
                          'ERROR']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()

            for key in self.results:
                r = self.results[key]
                writer.writerow({'ETA': r['eta'], 'ALPHA': r['alpha'],
                                 'LAMBDA': r['reg_lambda'],
                                 'BATCH SIZE': r['batch_size'],
                                 'EPOCHS': r['epochs'], 'ERROR': key})
